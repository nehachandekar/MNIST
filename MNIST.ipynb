{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb636ac",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92e5411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nehac\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\neighbors\\_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Score: 0.9933\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"KNN Score: %0.4f\"%model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb8d97",
   "metadata": {},
   "source": [
    "By applying .values.ravel() to y_train and y_test, you ensure that they are converted to 1-dimensional arrays, which should resolve the warning message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b9fa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Score: 0.9933\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to 1-dimensional arrays\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "# Initialize and train the K-Nearest Neighbors classifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy score of the KNN model on the test set\n",
    "print(\"KNN Score: %0.4f\" % model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9796c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create an SVM classifier model\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate the SVM model\n",
    "svm_accuracy = svm_model.score(X_test, y_test)\n",
    "print(\"SVM Accuracy: %.4f\" % svm_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115e114",
   "metadata": {},
   "source": [
    "In this code, we use a Support Vector Machine (SVM) classifier instead of the k-nearest neighbors algorithm. SVMs are another popular choice for classification tasks. We use a linear kernel and set the regularization parameter C to 1.0. After training the SVM model, we evaluate its accuracy on the test data and print the result.You can experiment with different kernels and parameters to see how they affect the SVM's performance on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa59a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9711\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "rf_accuracy = rf_model.score(X_test, y_test)\n",
    "print(\"Random Forest Accuracy: %.4f\" % rf_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2d8b6",
   "metadata": {},
   "source": [
    "In this code, we're using a Random Forest classifier, which is an ensemble learning method based on decision trees. We specify the number of trees in the forest using the n_estimators parameter (set to 100 in this example). After training the Random Forest model, we evaluate its accuracy on the test data and print the result.\n",
    "\n",
    "Random Forest classifiers often perform well on a variety of datasets, including MNIST, due to their ability to handle high-dimensional data and capture complex relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e388f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Accuracy: 0.9111\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train the Multinomial Naive Bayes model\n",
    "nb_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate the Multinomial Naive Bayes model\n",
    "nb_accuracy = nb_model.score(X_test, y_test)\n",
    "print(\"Multinomial Naive Bayes Accuracy: %.4f\" % nb_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392fcc5",
   "metadata": {},
   "source": [
    "In this code, we're using a Multinomial Naive Bayes classifier, which is suitable for classification with discrete features (like word counts for text classification). Even though the MNIST dataset contains pixel values, we can normalize them to a range of [0, 1] and still apply the Multinomial Naive Bayes classifier effectively.\n",
    "\n",
    "After splitting the data into training and testing sets, we create the Multinomial Naive Bayes model, train it with the training data, and evaluate its accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67326f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9689\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a Logistic Regression classifier model\n",
    "logreg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "logreg_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "logreg_accuracy = logreg_model.score(X_test, y_test)\n",
    "print(\"Logistic Regression Accuracy: %.4f\" % logreg_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf3948",
   "metadata": {},
   "source": [
    "In this code, we're using logistic regression, which is a linear classification algorithm. We normalize the features using Min-Max scaling, split the data into training and testing sets, create a logistic regression model, train it with the training data, and then evaluate its accuracy on the test data.\n",
    "\n",
    "Logistic regression is widely used for binary classification tasks but can also be extended to multiclass classification, making it suitable for the MNIST dataset, which has multiple classes (digits 0 through 9).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93d0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8578\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a Decision Tree classifier model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Decision Tree model\n",
    "dt_accuracy = dt_model.score(X_test, y_test)\n",
    "print(\"Decision Tree Accuracy: %.4f\" % dt_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b6b51",
   "metadata": {},
   "source": [
    "In this code, we're using a Decision Tree classifier, which is a popular and interpretable machine learning algorithm. Decision trees recursively split the data into subsets based on the most significant attribute, creating a tree-like structure. We then evaluate the accuracy of the model on the test data.\n",
    "\n",
    "Decision trees are capable of learning complex decision boundaries and can handle both numerical and categorical data, making them suitable for various classification tasks, including the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa532bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.9711\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier model\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "gb_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate the Gradient Boosting model\n",
    "gb_accuracy = gb_model.score(X_test, y_test)\n",
    "print(\"Gradient Boosting Accuracy: %.4f\" % gb_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7aff9d",
   "metadata": {},
   "source": [
    "In this code, we're using a Gradient Boosting classifier, which is an ensemble learning technique that builds a strong model by combining multiple weak models, typically decision trees. Gradient boosting iteratively trains new models to correct errors made by previous models, gradually improving accuracy. After splitting the data into training and testing sets, we create the Gradient Boosting classifier, train it with the training data, and then evaluate its accuracy on the test data.\n",
    "\n",
    "Gradient Boosting classifiers are known for their high predictive accuracy and robustness against overfitting, making them suitable for various classification tasks, including the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa450fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the Gaussian Naive Bayes model\n",
    "nb_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate the Gaussian Naive Bayes model\n",
    "nb_accuracy = nb_model.score(X_test, y_test)\n",
    "print(\"Gaussian Naive Bayes Accuracy: %.4f\" % nb_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcccf646",
   "metadata": {},
   "source": [
    "In this code, we're using a Gaussian Naive Bayes classifier, which assumes that the features follow a Gaussian distribution. Before training the model, we standardize the features to have a mean of 0 and a standard deviation of 1 using StandardScaler. Then, we split the data into training and testing sets, create the Gaussian Naive Bayes classifier, train it with the training data, and evaluate its accuracy on the test data.\n",
    "\n",
    "Naive Bayes classifiers are simple yet effective probabilistic classifiers that are particularly useful for datasets with many features like the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ba597a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with RBF Kernel Accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create an SVM classifier model with RBF kernel\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate the SVM model\n",
    "svm_accuracy = svm_model.score(X_test, y_test)\n",
    "print(\"SVM with RBF Kernel Accuracy: %.4f\" % svm_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c817c",
   "metadata": {},
   "source": [
    "In this code, we're using a Support Vector Machine (SVM) classifier with a radial basis function (RBF) kernel. SVM with an RBF kernel is a powerful classifier that can capture complex decision boundaries. Before training the model, we standardize the features to have a mean of 0 and a standard deviation of 1 using StandardScaler. Then, we split the data into training and testing sets, create the SVM classifier with an RBF kernel, train it with the training data, and evaluate its accuracy on the test data.\n",
    "\n",
    "SVM with an RBF kernel is particularly effective for high-dimensional datasets like the MNIST dataset and can achieve high accuracy in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0839b957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create an MLP classifier model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the MLP model\n",
    "mlp_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Evaluate the MLP model\n",
    "mlp_accuracy = mlp_model.score(X_test, y_test)\n",
    "print(\"MLP Classifier Accuracy: %.4f\" % mlp_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa47113",
   "metadata": {},
   "source": [
    "In this code, we're using a Multilayer Perceptron (MLP) classifier, which is a type of artificial neural network with multiple layers of nodes. MLP classifiers are capable of learning complex relationships in data and are widely used for classification tasks.\n",
    "\n",
    "Before training the model, we standardize the features to have a mean of 0 and a standard deviation of 1 using StandardScaler. Then, we split the data into training and testing sets, create the MLP classifier with a single hidden layer of 100 neurons, train it with the training data, and evaluate its accuracy on the test data.\n",
    "\n",
    "MLP classifiers are flexible and can handle various types of data, making them suitable for the MNIST dataset. They can achieve high accuracy but may require tuning of hyperparameters and careful preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d3c6222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\t\tAccuracy\n",
      "------------------------------\n",
      "K-Nearest Neighbors\t\t0.9778\n",
      "Support Vector Machine\t\t0.9822\n",
      "Random Forest\t\t0.9756\n",
      "Naive Bayes\t\t0.7778\n",
      "Multilayer Perceptron\t\t0.9844\n",
      "Decision Tree\t\t0.8533\n",
      "Gradient Boosting\t\t0.9711\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize all classifiers\n",
    "classifiers = {\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Support Vector Machine\": SVC(kernel='rbf', C=1.0, gamma='scale'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Multilayer Perceptron\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate all classifiers\n",
    "results = {}\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train.values.ravel())\n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Print the results\n",
    "print(\"Classifier\\t\\tAccuracy\")\n",
    "print(\"-\" * 30)\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}\\t\\t{accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c2cd0",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We define a dictionary called classifiers, which contains instances of various classifiers, such as K-Nearest Neighbors, Support Vector Machine, Random Forest, Naive Bayes, Multilayer Perceptron, Decision Tree, and Gradient Boosting.\n",
    "We train each classifier on the training data and evaluate its accuracy on the test data.\n",
    "Finally, we print out the accuracy of each classifier.\n",
    "This approach allows you to compare the performance of multiple classifiers on the same dataset. You can easily add or remove classifiers from the classifiers dictionary to include different algorithms or variations of the same algorithm with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b627b26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\t\tAccuracy\n",
      "------------------------------\n",
      "K-Nearest Neighbors\t\t0.9778\n",
      "Support Vector Machine\t\t0.9822\n",
      "Random Forest\t\t0.9756\n",
      "Naive Bayes\t\t0.7778\n",
      "Multilayer Perceptron\t\t0.9844\n",
      "Decision Tree\t\t0.8533\n",
      "Gradient Boosting\t\t0.9711\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize all classifiers\n",
    "classifiers = {\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Support Vector Machine\": SVC(kernel='rbf', C=1.0, gamma='scale'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Multilayer Perceptron\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate all classifiers\n",
    "results = {}\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train.values.ravel())\n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Print the results\n",
    "print(\"Classifier\\t\\tAccuracy\")\n",
    "print(\"-\" * 30)\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}\\t\\t{accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fec61",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We initialize multiple classifiers, including K-Nearest Neighbors, Support Vector Machine, Random Forest, Naive Bayes, Multilayer Perceptron, Decision Tree, and Gradient Boosting.\n",
    "We train each classifier on the training data and evaluate its accuracy on the test data.\n",
    "Finally, we print out the accuracy of each classifier.\n",
    "This approach allows you to compare the performance of multiple classifiers on the MNIST dataset. You can easily add or remove classifiers from the classifiers dictionary to include different algorithms or variations of the same algorithm with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "405c4022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\t\tAccuracy\n",
      "----------------------------------------\n",
      "K-Nearest Neighbors\t\t0.9778\n",
      "Support Vector Machine\t\t0.9822\n",
      "Random Forest\t\t0.9756\n",
      "Naive Bayes\t\t0.7778\n",
      "Multilayer Perceptron\t\t0.9844\n",
      "Decision Tree\t\t0.8533\n",
      "Gradient Boosting\t\t0.9711\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = datasets.load_digits()\n",
    "\n",
    "# Convert data into Pandas DataFrames\n",
    "X = pd.DataFrame(np.array(mnist.data))\n",
    "y = pd.DataFrame(mnist.target)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Support Vector Machine\": SVC(kernel='rbf', C=1.0, gamma='scale'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Multilayer Perceptron\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "results = {}\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train.values.ravel())\n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Print the results\n",
    "print(\"Classifier\\t\\tAccuracy\")\n",
    "print(\"-\" * 40)\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}\\t\\t{accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44275a",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We define a dictionary called classifiers, which contains instances of multiple classifiers, including K-Nearest Neighbors, Support Vector Machine, Random Forest, Naive Bayes, Multilayer Perceptron, Decision Tree, and Gradient Boosting.\n",
    "We train each classifier on the training data and evaluate its accuracy on the test data.\n",
    "Finally, we print out the accuracy of each classifier.\n",
    "This approach allows you to compare the performance of multiple classifiers on the same MNIST dataset. You can easily add or remove classifiers from the classifiers dictionary to include different algorithms or variations of the same algorithm with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a9aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
